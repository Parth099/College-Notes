# 1.2 Discrete Probability Distributions
Recall the definition of a Random Variable from [[Random Variable]]. 

For example suppose the experiment was flipping 2 coins at the same time.

$$\Omega = \{HH, HT, TH, TT\}$$
and we wish to consider only heads in this experiment. 
The random variable $X$ will assign 2 to a sample element like $HH$. 

A ***sample space*** is the set of all possible outcomes. 

For now the sample space is *finite*.

## Definitions 1.1
If a sample space is countable *finite* it is said to be ***discrete***. A sample space is denoted by $\Omega$. 
(See countable vs uncountable infinity )

The elements of a sample space $\Omega$ are known as ***outcomes***. Outcomes are lowercase letters.

A subset of a sample space $\Omega$ is known as an ***event***. Events are capital letters.

### Example A
A die is rolled. Let $x$ be the outcome of the experiment. The sample space and an Event then are:
$$
\begin{align*}
\Omega &= \{1, 2, 3, 4, 5, 6\} \\
E &= \{2, 4, 6\}
\end{align*}
$$
Notice $E \subset \Omega$.

We can also see that if the dice is fair $m(i) \frac{1}{6}$.

## Distribution Functions
A DF describes the assignment of probabilities.
### Definition 1.2
Let $X$ be a random var for a *discrete* sample space $\Omega$. 
A ***distribution function*** for $X$ is a real-valued function $m$ whose domain is $\Omega$ which satisfies the following:
1. $m(\omega) \geq 0$
2. $\sum_{\omega\in\Omega} m(\omega) = 1$
3. $0 \leq P(E) \leq 1$

Realize this now:
For any $E\subset\Omega$ we can define $P(E) = \sum_{\omega\in E} m(\omega)$
> The probability of an event $E$ is the sum its elements.

For disjoint events $E_1, E_2, \dotso$
See that:
$$P(E_1 \cup E_2 \cup \dotso) = \sum_{k} P(E_k)$$
### Example A
Consider an experiment where a coin is flipped twice.
Here are some possible *sample spaces*.
#### Proposed Sample Spaces
$$
\begin{align*}
\Omega_1 &= \{HH, HT, TH, TT\}\\
\Omega_2 &= \{0, 1, 2\}\\
\Omega_3 &= \{HH, HT, TT\}
\end{align*}
$$
#### Sample Space Explanations
1. Coins record in the order they are in. We must decide coin order
2. Number of Heads
3. #1 without Coin order

Book uses #1 for now.
The *distribution function* $m$ for now is $\frac{1}{4}$ for all outputs.

Say we wished to know what is the chance at-least one head shows up.
$$
\begin{align*}
E &= \{HH, HT, TH\} \\
P(E) &= \sum_{\omega\in E} m(\omega) \\
&= \boxed{\frac{3}{4}}
\end{align*}
$$

### Example B
Three people A, B, and C run for office. Only one can win. Suppose Both A & B have twice the chance of C of winning the election.
$$m(A) = m(B) = 2m(c)$$
Recall that $\sum_{\omega\in\Omega} m(\omega) = 1$, here $\Omega = \{A, B, C\}$

$$
\begin{align*}
\implies m(A) + m(B) + 2m(C) &= 1 \\
2m(C) + 2m(C) + m(C) &= 1 \\
				m(C) &= \frac{1}{5}
\end{align*}
$$
Knowing this we can solve many questions that deal with Events with **this** sample space.

## Set Theory & Prob Properties with Theorems
1. $P(E) \geq 0$ for every $E \subset Q$
2. $P(\Omega) = 1$
3. $E \subset F \subset \Omega \implies P(E) \leq P(F)$
4. $A \cap B = \emptyset \implies P(A \cup B) = P(A) + P(B)$ 
5. $P(A^C) = 1 - P(A)$, $A^C$ represents the complement of $A$

See that Prop 4 can be extended to $n$ disjoint sets. Prop 4 can also be used to prove Prop 5 as $\Omega = A \cup A^C$.

Recall from Set Theory that
$$\lvert A \cup B \rvert = \lvert A \rvert + \lvert B \rvert - \lvert A \cap B \rvert$$
The same logic applies to PT
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

Here is 3 sets. The formula *should* be discernible
![[3venn.jpg]]



Reading Save - Page 33/520 
Topic Theorem 1.3

### Uniform Distribution 

The Uniform Distribution is the function on the sample space $\Omega$ with $n$ elements that assigns the probability $\frac{1}{n}$ to each outcome.

Question:
![[classP_1.png]]
See that the sample space is like:
$$\Omega_0 = \{H, TH, TTH, \dotso, T\dotso H\}$$
This is not uniform but notice it is a distribution. 
Verify that it is a valid distribution. 