# Locality and Memory Hierarchy 
![[nice_pyramid_mem_hire.png]]

The closer we get to the top the faster the memory is. 
The caches(L1, L2, L3) work in unison:

CPU registers will hold `words` retrieved from L1 which has retrieved from L2 which has retrieved from L3.

Recall what a `word` is: **2** bytes

Now L3 has retrieved from main memory which has retrieved from secondary storage(s). 

> **Note**: Any item before memory has DMA by the CPU (direct memory access). Anything below Main Memory RAM requires permissions and is indirect access. (Virtual Memory will be discussed in 3207)

The caches are SRAM while main memory is DRAM. 

## Cache Hierarchy
![[i7_caches.png]]

See that each core has its own components (registers, caches)

## Access Time Perspective
![[access_time.png]]

The left table represents a fake CPU time while the right side is scaled time. 

## Locality 
```c
for(i = 0; i < SIZE; i++){
	sum += A[i];
}
```

Variables: `i`, `sum`, `A`

Notice that `i` and `sum` is altered `size` times in total while each array element is only accessed *once*. 

Thus, `i` and `sum` are stored with better temporal locality (maybe in the cache). 
However the array will be stored with bad temporal locality but good spatial so it can be accessed faster but not to take up previous cache space. It will end up in the RAM likely. 

## Cache Sizes
Each cache has an *array* of sets where each set contains one or more lines. Each line contains a valid bit some tags and then a block of data. 

![[example_cache_sets.png]]

We can think of each cache as a building:

+ S: Floors
+ E: Number of Rooms per floor
+ B: Seats per Room

Why are sets in power of 2? So we can address them in bits. 
This addressing is called "Set Index bits", the amount of bits required is that power of 2 for the number of sets.

This also works for B. This is known as block offset bits.

## In Depth Analysis: Generic Cache Memory Organization
Consider a system where $n$ bits form $2^n$ unique memory addresses. Then there are $S = 2^s$ cache sets. Each cache set contains $E$ lines and each line contains $B = 2^n$  data blocks. Each line contains a valid-bit that determines if each line has valid data.  

There are also $t = m - (b + s)$ bits that uniquely identify the block stored in the cache line. 

Each cache can be characterized by a tuple of information (S, E, B, m):
$$
C = S \times E \times B
$$

1. C : Cache Size (in terms of blocks)
2. S : Number of Cache Sets
3. E : Number of lines per Cache Set
4. B : Number of Blocks per line

## Practice Problems For *Locality*
### Problem 1
Suppose you know that Cache 1 has $m = 32$, an aggregate block size of 1,024, 4 blocks per line and one line per Cache set. Find $S, t, s, b$

First we Find $S$.
S is the number of Cache Sets. We know that the cache size is 1,024 with 4 blocks per line and 1 line per set.
Thus we have
$$
1,024 = S \times 4 \times 1
$$
Thus $S = 256$.
Then $s = 8$ since it takes 8 bits to address 256 cache sets.

We also know that $b = 2$ since it will only take 2 bits address 4 blocks.

Then $t = m - (b+s) = 32 - (8+2) = 22$
### Problem 2
Suppose you know 
$$
\begin{align}
	m &= 32 \\
	C &= 1024 \\
	B &= 8 \\
	E &= 4
\end{align}
$$

Then the number of cache sets is 32 which take 6 bits to address them.
$$
\begin{align}
	S &= 32 \\
	s &= 5 \\
\end{align}
$$
We also need 3 bits to address each block inside a line. 
Then each unique block tag will be $t = 32 - (5 + 3) = 24$


# Caching
## 2D array example
Snippet 1
```c
sum = 0
for(i = 0; i < NROWS; i++)
	for(j = 0; j < NROWS; j++)
		sum += A[i][j]
```

Snippet 2
```c
sum = 0
for(i = 0; i < NROWS; i++)
	for(j = 0; j < NROWS; j++)
		sum += A[j][i]
```

The only difference here is the array access `A[i][j]` vs `A[j][i]`
One is a row by col traversal while one is a col by row traversal. 

Snippet 1 is *faster*. This is because we can cache the array and visit it fully before moving to the next array. 

## Cache Hit and Miss
Suppose that reading from slow memory is 100x slower than reading from a faster memory. 

+ A hit takes 1x access time 
+ A miss takes 100x access time

A hit is when a CPU finds its next (proper) target in the Cache. Having to look for it in the memory is very slow. 

**Average Access Time**
```
avg = p(hit) * hit_time + p(miss) * miss_time
```
It looks like an expected value calculation. 

Suppose `p(hit) = .97`
Then, 

```c
avg = (0.97) * 1 + (0.03) * (1 + 100)
avg = 4 units
```

Notice how it says `1+100` and not just `100`. We `+1` since we have wasted that `1x` time from not hitting the first time. 

If our `p(hit) = .99` the average time for access will be *2 units*.

In reality, to increase cache hit rates we must increase cache sizes. 

<!-- 
This marks the end of CIS2107
Ended at 12:05 on Monday 4_25_22  
-->