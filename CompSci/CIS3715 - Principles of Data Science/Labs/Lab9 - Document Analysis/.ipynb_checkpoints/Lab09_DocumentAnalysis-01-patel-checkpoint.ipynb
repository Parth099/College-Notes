{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Document Analysis\n",
    "\n",
    "In this assignment, we will learn how to do document classification and clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example\n",
    "\n",
    "In this example, we use [20newsgroups](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) dataset. Each sample is a document and there are totally 20 classes. \n",
    "\n",
    "### 1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data target labels: [7 4 4 ... 3 1 8]\n",
      "Train data target names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "#training samples: 11314\n",
      "#testing samples: 7532\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(\"Train data target labels: {}\".format(data_train.target))\n",
    "print(\"Train data target names: {}\".format(data_train.target_names))\n",
    "\n",
    "print('#training samples: {}'.format(len(data_train.data)))\n",
    "print('#testing samples: {}'.format(len(data_test.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Represent documents with TF-IDF represention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631) (7532, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#TF-IDF representation for each document\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_train_vectors = vectorizer.fit_transform(data_train.data)\n",
    "data_test_vectors = vectorizer.transform(data_test.data) \n",
    "\n",
    "print(data_train_vectors.shape, data_test_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use KNN to do document classification\n",
    "\n",
    "Here, we use the cross-validation method to select $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16846385009722467\n",
      "{'n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "Xtr = data_train_vectors\n",
    "Ytr = data_train.target\n",
    "\n",
    "Xte = data_test_vectors\n",
    "Yte = data_test.target\n",
    "\n",
    "k_range = range(1, 5)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "clf_knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "grid = GridSearchCV(clf_knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use Logistic Regression to do document classification\n",
    "Here, we also use the cross-validation method to select the regularization coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghc/Software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/ghc/Software/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 8}\n",
      "0.6889272437599575 0.6778761181105242 0.6889272437599575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "#=====training with cross validation======\n",
    "coeff = range(1, 10)\n",
    "param_grid = dict(C=coeff)\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l2')\n",
    "\n",
    "grid = GridSearchCV(clf_lr, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(Xtr, Ytr)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "#=====testing======\n",
    "clf_lr = LogisticRegression(penalty='l2', C=grid.best_params_['C'])\n",
    "clf_lr.fit(Xtr, Ytr)\n",
    "\n",
    "y_pred = clf_lr.predict(Xte)\n",
    "\n",
    "acc = accuracy_score(Yte, y_pred)\n",
    "macro_f1 = f1_score(Yte, y_pred, average='macro')\n",
    "micro_f1 = f1_score(Yte, y_pred, average='micro')\n",
    "\n",
    "print(acc, macro_f1, micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task: Document Classification and Clustering\n",
    "\n",
    "In this task, we are going to use [BBCNews](BBC_News_Train.csv) dataset. There are 1490 articles from 5 topics, including tech, business, sport, entertainment, politics. \n",
    "\n",
    "* Task 1: Please use KNN and logistic regression to do classification, and compare their performance.\n",
    "\n",
    "* Task 2: Please use K-means to partition this dataset into 5 clusters and find the representative words in each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data and represent it with TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use KNN to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use Logistic Regression to do document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Use K-means to do document clustering and find the 10 most representative words in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
